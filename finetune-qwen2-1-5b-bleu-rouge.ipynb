{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8534316,"sourceType":"datasetVersion","datasetId":5097499},{"sourceId":8534337,"sourceType":"datasetVersion","datasetId":5097515},{"sourceId":8551319,"sourceType":"datasetVersion","datasetId":5110007},{"sourceId":8694656,"sourceType":"datasetVersion","datasetId":5214082},{"sourceId":8694962,"sourceType":"datasetVersion","datasetId":5214262},{"sourceId":8760754,"sourceType":"datasetVersion","datasetId":5263605}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nmajor_version, minor_version = torch.cuda.get_device_capability()\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\nif major_version >= 8:\n    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\nelse:\n    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n    !pip install --no-deps xformers trl peft accelerate bitsandbytes\npass\n!pip install triton transformers\n!pip install -U datasets\n!pip install --pre -U xformers ##### this take some time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****Note** Restart the Kernal after package installation**","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HFToken\")\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T21:40:05.714498Z","iopub.execute_input":"2024-06-22T21:40:05.714853Z","iopub.status.idle":"2024-06-22T21:40:05.873872Z","shell.execute_reply.started":"2024-06-22T21:40:05.714822Z","shell.execute_reply":"2024-06-22T21:40:05.873159Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nfrom IPython.display import display_markdown\nmax_seq_length = 1200 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/Qwen2-1.5B\",  \n]  #### loadin llama 3 model in 4 bit to fine tune","metadata":{"execution":{"iopub.status.busy":"2024-06-22T21:40:05.877900Z","iopub.execute_input":"2024-06-22T21:40:05.878154Z","iopub.status.idle":"2024-06-22T21:40:26.197289Z","shell.execute_reply.started":"2024-06-22T21:40:05.878131Z","shell.execute_reply":"2024-06-22T21:40:26.196455Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2024-06-22 21:40:15.994592: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-22 21:40:15.994732: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-22 21:40:16.146976: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Qwen2-1.5B-Instruct\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = False,\n    token = secret_value_0\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:04:05.582914Z","iopub.execute_input":"2024-06-22T22:04:05.583477Z","iopub.status.idle":"2024-06-22T22:04:28.330369Z","shell.execute_reply.started":"2024-06-22T22:04:05.583432Z","shell.execute_reply":"2024-06-22T22:04:28.328850Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2240f1ec191d4d13b0f9143eb42787a6"}},"metadata":{}},{"name":"stdout","text":"==((====))==  Unsloth: Fast Qwen2 patching release 2024.6\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. Xformers = 0.0.27.dev792. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b467b5c2a024ad097ce0d88dfb07d6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3219e69a58d244b3a11e4ab6698078be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84b05e56f4614437a93634aac2fbd4ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b15989ae1421417b8aa5b625a4677bb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"106d3b69a8264593a8241ce478fcc875"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e12ee4e6c607470c868160d9dccc6004"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/367 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f50633193e0c4907ac036a6825e23f8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43222fd120a64f66a5562d476d9a6b2d"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:04:28.332926Z","iopub.execute_input":"2024-06-22T22:04:28.333716Z","iopub.status.idle":"2024-06-22T22:04:34.244020Z","shell.execute_reply.started":"2024-06-22T22:04:28.333671Z","shell.execute_reply":"2024-06-22T22:04:34.242846Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"Unsloth 2024.6 patched 28 layers with 0 QKV layers, 28 O layers and 0 MLP layers.\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\nfrom datasets import load_dataset\nimport numpy as np\nseed = 42\nnp.random.seed(seed)\nimport pandas as pd\ndf=pd.read_excel('/kaggle/input/lllllllo/text2cypher.xlsx')\n\n\ndef format_schema(schema):\n    return f\"\"\"You are an expert in querying Neo4j graph databases using the Cypher query language. You are provided with a specific graph schema and your task is to generate accurate and syntactically correct Cypher queries based on questions about this graph. The graph schema is as follows <schema>:\\n{schema}\\n</schema>\"\"\"\n\n# Apply the formatting function to the 'new_schema' column\ndf['Schema'] = df['Schema'].apply(lambda x: format_schema(x))\n\nfrom datasets import Dataset, DatasetDict\ndf = df.sample(frac=1).reset_index(drop=True)\ntrain_df = df.iloc[:66]\nval_df = df.iloc[66:]\ntrain_df\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ndataset = DatasetDict()\n# Create a Hugging Face Dataset from the dictionary\ndataset['train'] = train_dataset\ndataset['val'] = val_dataset\ndataset_subset = dataset[\"train\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:04:34.245317Z","iopub.execute_input":"2024-06-22T22:04:34.245654Z","iopub.status.idle":"2024-06-22T22:04:34.312161Z","shell.execute_reply.started":"2024-06-22T22:04:34.245625Z","shell.execute_reply":"2024-06-22T22:04:34.310979Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"dataset_subset['Schema'][0]","metadata":{"execution":{"iopub.status.busy":"2024-06-22T21:43:25.698707Z","iopub.execute_input":"2024-06-22T21:43:25.699406Z","iopub.status.idle":"2024-06-22T21:43:25.709741Z","shell.execute_reply.started":"2024-06-22T21:43:25.699373Z","shell.execute_reply":"2024-06-22T21:43:25.708766Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'You are an expert in querying Neo4j graph databases using the Cypher query language. You are provided with a specific graph schema and your task is to generate accurate and syntactically correct Cypher queries based on questions about this graph. The graph schema is as follows <schema>:\\nNode properties:\\nInvoice {name: STRING}\\nClient {name: STRING}\\nSeller {name: STRING}\\nTotalAmount {name: INTEGER}\\nProductDescription {name: STRING}\\nQuantity {name: STRING}\\nUnitPrice {name: STRING}\\nTotal {name: STRING}\\nRelationship properties:\\n\\nThe relationships:\\n(:Invoice)-[:HAS_CLIENT]->(:Client)\\n(:Invoice)-[:HAS_SELLER]->(:Seller)\\n(:Invoice)-[:HAS_TOTAL_AMOUNT]->(:TotalAmount)\\n(:Invoice)-[:HAS_PRODUCT]->(:ProductDescription)\\n(:ProductDescription)-[:HAS_QUANTITY]->(:Quantity)\\n(:ProductDescription)-[:HAS_UNIT_PRICE]->(:UnitPrice)\\n(:ProductDescription)-[:HAS_TOTAL]->(:Total)\\n</schema>'"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\ndata2 = load_dataset(\"open-llm-leaderboard/details_aloobun__Cypher-7B\",\n    \"harness_winogrande_5\")","metadata":{"execution":{"iopub.status.busy":"2024-06-22T21:43:49.316247Z","iopub.execute_input":"2024-06-22T21:43:49.316834Z","iopub.status.idle":"2024-06-22T21:43:51.009442Z","shell.execute_reply.started":"2024-06-22T21:43:49.316792Z","shell.execute_reply":"2024-06-22T21:43:51.008637Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides the Question. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    \n    instructions = examples[\"Schema\"]\n    inputs       = examples[\"Question\"]\n    outputs      = examples[\"Answer\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\n\ndataset = dataset_subset.map(formatting_prompts_func, batched = True,)\nval_dataset=val_dataset.map(formatting_prompts_func,batched=True,)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:04:34.314082Z","iopub.execute_input":"2024-06-22T22:04:34.315750Z","iopub.status.idle":"2024-06-22T22:04:35.002706Z","shell.execute_reply.started":"2024-06-22T22:04:34.315720Z","shell.execute_reply":"2024-06-22T22:04:35.001720Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/66 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aca052601df74064958731b2e63ea0de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"003585b767904410874b1f8c0315406d"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install evaluate\nimport evaluate\nbleu=evaluate.load('bleu')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_metric\nmetric = load_metric('bleu')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T21:45:10.445939Z","iopub.execute_input":"2024-06-22T21:45:10.446710Z","iopub.status.idle":"2024-06-22T21:45:12.874633Z","shell.execute_reply.started":"2024-06-22T21:45:10.446676Z","shell.execute_reply":"2024-06-22T21:45:12.873732Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_172/576045966.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n  metric = load_metric('bleu')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcff2474a5324c4ba7513093c822acc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"130f28e4a1ee46149a8a2d6e9a011202"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for bleu contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bleu.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"}]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=-1)\n    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    for i,pred in enumerate(decoded_preds):\n        x=pred.find(\"### Response\")\n        decoded_preds[i]=pred[x+len(\"### Response \"):]\n    for i,lab in enumerate(decoded_labels):\n        x=lab.find(\"### Response\")\n        decoded_labels[i]=lab[x+len(\"### Response \"):]\n\n       \n    return bleu.compute(predictions=decoded_preds, references=decoded_labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:04:54.261207Z","iopub.execute_input":"2024-06-22T22:04:54.261982Z","iopub.status.idle":"2024-06-22T22:04:54.271822Z","shell.execute_reply.started":"2024-06-22T22:04:54.261949Z","shell.execute_reply":"2024-06-22T22:04:54.270078Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    eval_dataset = val_dataset,\n    compute_metrics=compute_metrics,\n    dataset_text_field = \"text\",\n    max_seq_length = 1200,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        auto_find_batch_size=True,\n        gradient_accumulation_steps = 2,\n        warmup_steps = 5,\n        max_steps = 80,\n        learning_rate = 1e-3,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 1,\n        \n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"cosine\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        load_best_model_at_end = True,\n        eval_accumulation_steps = 1,\n        evaluation_strategy = \"steps\",\n        eval_steps = 1,\n    ),\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:05:07.075617Z","iopub.execute_input":"2024-06-22T22:05:07.076465Z","iopub.status.idle":"2024-06-22T22:05:09.630514Z","shell.execute_reply.started":"2024-06-22T22:05:07.076435Z","shell.execute_reply":"2024-06-22T22:05:09.629362Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/66 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18a98204c6544bac91512de569a7ebcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de58dda7a75d43c48929a15a4289237a"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:05:11.083666Z","iopub.execute_input":"2024-06-22T22:05:11.084049Z","iopub.status.idle":"2024-06-22T22:21:39.482507Z","shell.execute_reply.started":"2024-06-22T22:05:11.084016Z","shell.execute_reply":"2024-06-22T22:21:39.481526Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 66 | Num Epochs = 20\nO^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 2\n\\        /    Total batch size = 16 | Total steps = 80\n \"-____-\"     Number of trainable parameters = 2,179,072\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [80/80 16:19, Epoch 17/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Precisions</th>\n      <th>Brevity Penalty</th>\n      <th>Length Ratio</th>\n      <th>Translation Length</th>\n      <th>Reference Length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.752100</td>\n      <td>1.725660</td>\n      <td>0.403727</td>\n      <td>[0.52978515625, 0.4408284023668639, 0.3665338645418327, 0.3103621730382294]</td>\n      <td>1.000000</td>\n      <td>1.681445</td>\n      <td>2048</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.763600</td>\n      <td>1.622938</td>\n      <td>0.407806</td>\n      <td>[0.5320293398533007, 0.44493827160493826, 0.371072319201995, 0.3148614609571788]</td>\n      <td>1.000000</td>\n      <td>1.678982</td>\n      <td>2045</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.664900</td>\n      <td>1.483271</td>\n      <td>0.412168</td>\n      <td>[0.5329428989751098, 0.44701823558403153, 0.375311100049776, 0.32277526395173456]</td>\n      <td>1.000000</td>\n      <td>1.682266</td>\n      <td>2049</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.484200</td>\n      <td>1.350304</td>\n      <td>0.419407</td>\n      <td>[0.53369140625, 0.4521696252465483, 0.3844621513944223, 0.3335010060362173]</td>\n      <td>1.000000</td>\n      <td>1.681445</td>\n      <td>2048</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.390400</td>\n      <td>1.158232</td>\n      <td>0.438804</td>\n      <td>[0.5419921875, 0.46794871794871795, 0.40587649402390436, 0.36016096579476864]</td>\n      <td>1.000000</td>\n      <td>1.681445</td>\n      <td>2048</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.169000</td>\n      <td>0.925993</td>\n      <td>0.442712</td>\n      <td>[0.54296875, 0.46893491124260356, 0.4108565737051793, 0.3672032193158954]</td>\n      <td>1.000000</td>\n      <td>1.681445</td>\n      <td>2048</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.924200</td>\n      <td>0.724615</td>\n      <td>0.452149</td>\n      <td>[0.5464774951076321, 0.4772727272727273, 0.4216566866267465, 0.3800403225806452]</td>\n      <td>1.000000</td>\n      <td>1.678161</td>\n      <td>2044</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.727000</td>\n      <td>0.529960</td>\n      <td>0.456244</td>\n      <td>[0.5441678867740362, 0.4785608674223756, 0.4275759084121453, 0.3891402714932127]</td>\n      <td>1.000000</td>\n      <td>1.682266</td>\n      <td>2049</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.543900</td>\n      <td>0.386114</td>\n      <td>0.485316</td>\n      <td>[0.5657229524772498, 0.5056179775280899, 0.45923632610939114, 0.42231491136600624]</td>\n      <td>1.000000</td>\n      <td>1.623974</td>\n      <td>1978</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.393300</td>\n      <td>0.288402</td>\n      <td>0.495028</td>\n      <td>[0.578076525336091, 0.5156739811912225, 0.4677930306230201, 0.43062966915688367]</td>\n      <td>1.000000</td>\n      <td>1.587849</td>\n      <td>1934</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.269200</td>\n      <td>0.236245</td>\n      <td>0.498025</td>\n      <td>[0.5861522198731501, 0.5181623931623932, 0.4703023758099352, 0.4306768558951965]</td>\n      <td>1.000000</td>\n      <td>1.553366</td>\n      <td>1892</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.210800</td>\n      <td>0.193814</td>\n      <td>0.502291</td>\n      <td>[0.5879556259904913, 0.5242925787506674, 0.4749055585536967, 0.4348063284233497]</td>\n      <td>1.000000</td>\n      <td>1.554187</td>\n      <td>1893</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.168300</td>\n      <td>0.172617</td>\n      <td>0.507101</td>\n      <td>[0.5858111632759521, 0.5282024248813917, 0.48161960575386253, 0.4437264404954227]</td>\n      <td>1.000000</td>\n      <td>1.573892</td>\n      <td>1917</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.142600</td>\n      <td>0.163504</td>\n      <td>0.513401</td>\n      <td>[0.5871369294605809, 0.5319706498951782, 0.4894067796610169, 0.4544967880085653]</td>\n      <td>1.000000</td>\n      <td>1.582923</td>\n      <td>1928</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.121900</td>\n      <td>0.151803</td>\n      <td>0.510684</td>\n      <td>[0.5876556016597511, 0.529874213836478, 0.4851694915254237, 0.45021413276231265]</td>\n      <td>1.000000</td>\n      <td>1.582923</td>\n      <td>1928</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.121300</td>\n      <td>0.144088</td>\n      <td>0.528904</td>\n      <td>[0.5939614783966684, 0.5444502893214098, 0.5077086656034024, 0.47662547017732404]</td>\n      <td>1.000000</td>\n      <td>1.577176</td>\n      <td>1921</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.129600</td>\n      <td>0.140294</td>\n      <td>0.530460</td>\n      <td>[0.5928050052137643, 0.5442571127502634, 0.5111821086261981, 0.48008611410118407]</td>\n      <td>1.000000</td>\n      <td>1.574713</td>\n      <td>1918</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.124300</td>\n      <td>0.135534</td>\n      <td>0.533370</td>\n      <td>[0.6034755134281201, 0.5508249068653539, 0.5099515868746638, 0.47743338771071236]</td>\n      <td>1.000000</td>\n      <td>1.559113</td>\n      <td>1899</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.100000</td>\n      <td>0.129730</td>\n      <td>0.538063</td>\n      <td>[0.6072186836518046, 0.5547210300429185, 0.5151843817787418, 0.4830043859649123]</td>\n      <td>1.000000</td>\n      <td>1.546798</td>\n      <td>1884</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.117300</td>\n      <td>0.125030</td>\n      <td>0.544329</td>\n      <td>[0.615301724137931, 0.5615468409586056, 0.5209251101321586, 0.48775055679287305]</td>\n      <td>1.000000</td>\n      <td>1.523810</td>\n      <td>1856</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.087800</td>\n      <td>0.122062</td>\n      <td>0.556734</td>\n      <td>[0.6245231607629428, 0.5735537190082645, 0.5337047353760446, 0.5025352112676056]</td>\n      <td>1.000000</td>\n      <td>1.506568</td>\n      <td>1835</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.101000</td>\n      <td>0.117432</td>\n      <td>0.566578</td>\n      <td>[0.6325068870523416, 0.5844011142061282, 0.5442253521126761, 0.5122507122507123]</td>\n      <td>1.000000</td>\n      <td>1.490148</td>\n      <td>1815</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.077300</td>\n      <td>0.115347</td>\n      <td>0.567996</td>\n      <td>[0.6338495575221239, 0.5850111856823266, 0.5463800904977375, 0.5137299771167048]</td>\n      <td>1.000000</td>\n      <td>1.484401</td>\n      <td>1808</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.078000</td>\n      <td>0.114784</td>\n      <td>0.571593</td>\n      <td>[0.6387959866220736, 0.589064261555806, 0.5496009122006842, 0.5161476355247981]</td>\n      <td>1.000000</td>\n      <td>1.472906</td>\n      <td>1794</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.086800</td>\n      <td>0.111915</td>\n      <td>0.589525</td>\n      <td>[0.64501679731243, 0.6047565118912798, 0.5715922107674685, 0.5417149478563151]</td>\n      <td>1.000000</td>\n      <td>1.466338</td>\n      <td>1786</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.078700</td>\n      <td>0.110496</td>\n      <td>0.594429</td>\n      <td>[0.6490438695163104, 0.6092150170648464, 0.5771001150747986, 0.5471478463329453]</td>\n      <td>1.000000</td>\n      <td>1.459770</td>\n      <td>1778</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.092800</td>\n      <td>0.110883</td>\n      <td>0.602723</td>\n      <td>[0.652542372881356, 0.616, 0.5878612716763005, 0.5584795321637427]</td>\n      <td>1.000000</td>\n      <td>1.453202</td>\n      <td>1770</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.063100</td>\n      <td>0.110288</td>\n      <td>0.610303</td>\n      <td>[0.6630434782608695, 0.6255787037037037, 0.5936768149882904, 0.5633886255924171]</td>\n      <td>1.000000</td>\n      <td>1.435140</td>\n      <td>1748</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.076600</td>\n      <td>0.111654</td>\n      <td>0.610282</td>\n      <td>[0.6622781911848884, 0.6247828604516502, 0.5934387814879907, 0.5649081209247184]</td>\n      <td>1.000000</td>\n      <td>1.434319</td>\n      <td>1747</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.080300</td>\n      <td>0.111193</td>\n      <td>0.610844</td>\n      <td>[0.6643638457109959, 0.6266744321490972, 0.5928108426635239, 0.5641025641025641]</td>\n      <td>1.000000</td>\n      <td>1.426108</td>\n      <td>1737</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.070300</td>\n      <td>0.106510</td>\n      <td>0.615124</td>\n      <td>[0.6660929432013769, 0.6297156123041208, 0.5977686435701702, 0.5710041592394534]</td>\n      <td>1.000000</td>\n      <td>1.431034</td>\n      <td>1743</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.068300</td>\n      <td>0.103664</td>\n      <td>0.620687</td>\n      <td>[0.667621776504298, 0.6347826086956522, 0.6058651026392962, 0.5780415430267062]</td>\n      <td>1.000000</td>\n      <td>1.432677</td>\n      <td>1745</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.073800</td>\n      <td>0.103735</td>\n      <td>0.626142</td>\n      <td>[0.6710677382319173, 0.6387921022067363, 0.6110458284371327, 0.5868014268727705]</td>\n      <td>1.000000</td>\n      <td>1.430213</td>\n      <td>1742</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.058600</td>\n      <td>0.102208</td>\n      <td>0.622805</td>\n      <td>[0.6733870967741935, 0.6375291375291375, 0.6055424528301887, 0.5787589498806682]</td>\n      <td>1.000000</td>\n      <td>1.425287</td>\n      <td>1736</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.060600</td>\n      <td>0.100468</td>\n      <td>0.624153</td>\n      <td>[0.6737875288683602, 0.639018691588785, 0.607565011820331, 0.5801435406698564]</td>\n      <td>1.000000</td>\n      <td>1.422003</td>\n      <td>1732</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.052200</td>\n      <td>0.099700</td>\n      <td>0.627635</td>\n      <td>[0.6733755031627372, 0.6410703897614892, 0.6121247792819305, 0.5872543180464562]</td>\n      <td>1.000000</td>\n      <td>1.427750</td>\n      <td>1739</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.063400</td>\n      <td>0.100215</td>\n      <td>0.622096</td>\n      <td>[0.6710601719197707, 0.6359420289855072, 0.6052785923753665, 0.5798219584569733]</td>\n      <td>1.000000</td>\n      <td>1.432677</td>\n      <td>1745</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.049700</td>\n      <td>0.101918</td>\n      <td>0.620419</td>\n      <td>[0.6697142857142857, 0.6341040462427746, 0.6035087719298246, 0.5781065088757397]</td>\n      <td>1.000000</td>\n      <td>1.436782</td>\n      <td>1750</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.047800</td>\n      <td>0.103764</td>\n      <td>0.620636</td>\n      <td>[0.6670461013090495, 0.6332757628094415, 0.6051252184041933, 0.5804360636417207]</td>\n      <td>1.000000</td>\n      <td>1.442529</td>\n      <td>1757</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.046300</td>\n      <td>0.105948</td>\n      <td>0.620382</td>\n      <td>[0.6662876634451392, 0.6325474410580794, 0.6050029086678301, 0.5809299587992937]</td>\n      <td>1.000000</td>\n      <td>1.444171</td>\n      <td>1759</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.075300</td>\n      <td>0.107068</td>\n      <td>0.622542</td>\n      <td>[0.6685681688533942, 0.634737449509521, 0.6071220081727963, 0.5829887773183697]</td>\n      <td>1.000000</td>\n      <td>1.439245</td>\n      <td>1753</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.047100</td>\n      <td>0.108178</td>\n      <td>0.626331</td>\n      <td>[0.6712485681557846, 0.6373117033603708, 0.6107854630715123, 0.5889679715302492]</td>\n      <td>1.000000</td>\n      <td>1.433498</td>\n      <td>1746</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.050700</td>\n      <td>0.108479</td>\n      <td>0.626782</td>\n      <td>[0.6706758304696449, 0.6384704519119351, 0.611957796014068, 0.5889679715302492]</td>\n      <td>1.000000</td>\n      <td>1.433498</td>\n      <td>1746</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.046600</td>\n      <td>0.108512</td>\n      <td>0.626304</td>\n      <td>[0.6714449541284404, 0.6380510440835266, 0.6109154929577465, 0.5878859857482185]</td>\n      <td>1.000000</td>\n      <td>1.431856</td>\n      <td>1744</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.049700</td>\n      <td>0.107599</td>\n      <td>0.625240</td>\n      <td>[0.6735751295336787, 0.6371578334304019, 0.6087212728344137, 0.5849731663685152]</td>\n      <td>1.000000</td>\n      <td>1.426108</td>\n      <td>1737</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.036500</td>\n      <td>0.107643</td>\n      <td>0.628592</td>\n      <td>[0.6760969976905312, 0.6401869158878505, 0.6122931442080378, 0.5891148325358851]</td>\n      <td>1.000000</td>\n      <td>1.422003</td>\n      <td>1732</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.040400</td>\n      <td>0.108188</td>\n      <td>0.625184</td>\n      <td>[0.6731875719217492, 0.6367869615832363, 0.6089517078916372, 0.5852205005959475]</td>\n      <td>1.000000</td>\n      <td>1.426929</td>\n      <td>1738</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.043100</td>\n      <td>0.108115</td>\n      <td>0.621374</td>\n      <td>[0.6697247706422018, 0.632830626450116, 0.6050469483568075, 0.581353919239905]</td>\n      <td>1.000000</td>\n      <td>1.431856</td>\n      <td>1744</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.042100</td>\n      <td>0.108693</td>\n      <td>0.616074</td>\n      <td>[0.6636363636363637, 0.6275862068965518, 0.6, 0.5764705882352941]</td>\n      <td>1.000000</td>\n      <td>1.444992</td>\n      <td>1760</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.055800</td>\n      <td>0.109695</td>\n      <td>0.610047</td>\n      <td>[0.6602600339174675, 0.6214979988564894, 0.5928282244071718, 0.569338794616735]</td>\n      <td>1.000000</td>\n      <td>1.452381</td>\n      <td>1769</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.039000</td>\n      <td>0.111019</td>\n      <td>0.610096</td>\n      <td>[0.6617563739376771, 0.6217765042979942, 0.592463768115942, 0.5683284457478006]</td>\n      <td>1.000000</td>\n      <td>1.449097</td>\n      <td>1765</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.038500</td>\n      <td>0.112774</td>\n      <td>0.613770</td>\n      <td>[0.6619479048697622, 0.6254295532646048, 0.5973348783314021, 0.5738569753810082]</td>\n      <td>1.000000</td>\n      <td>1.449918</td>\n      <td>1766</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.037100</td>\n      <td>0.114931</td>\n      <td>0.613591</td>\n      <td>[0.6610073571024335, 0.6250715512306811, 0.5975680370584829, 0.5741066198008201]</td>\n      <td>1.000000</td>\n      <td>1.450739</td>\n      <td>1767</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.030400</td>\n      <td>0.117161</td>\n      <td>0.612054</td>\n      <td>[0.6604519774011299, 0.624, 0.5953757225433526, 0.5719298245614035]</td>\n      <td>1.000000</td>\n      <td>1.453202</td>\n      <td>1770</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.033300</td>\n      <td>0.120150</td>\n      <td>0.608168</td>\n      <td>[0.6578354002254791, 0.6197263397947549, 0.591118800461361, 0.5676779463243874]</td>\n      <td>1.000000</td>\n      <td>1.456486</td>\n      <td>1774</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.035000</td>\n      <td>0.122755</td>\n      <td>0.601725</td>\n      <td>[0.6521008403361345, 0.6141643059490085, 0.5845272206303725, 0.56]</td>\n      <td>1.000000</td>\n      <td>1.465517</td>\n      <td>1785</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.035900</td>\n      <td>0.124328</td>\n      <td>0.603825</td>\n      <td>[0.6510067114093959, 0.6153846153846154, 0.5881006864988558, 0.5642361111111112]</td>\n      <td>1.000000</td>\n      <td>1.467980</td>\n      <td>1788</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.029500</td>\n      <td>0.125701</td>\n      <td>0.605440</td>\n      <td>[0.6504745951982133, 0.616600790513834, 0.5905197030268418, 0.5673021374927787]</td>\n      <td>1.000000</td>\n      <td>1.470443</td>\n      <td>1791</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.034100</td>\n      <td>0.127255</td>\n      <td>0.602716</td>\n      <td>[0.6497206703910614, 0.6141242937853107, 0.5874285714285714, 0.5630057803468208]</td>\n      <td>1.000000</td>\n      <td>1.469622</td>\n      <td>1790</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.033100</td>\n      <td>0.128398</td>\n      <td>0.605320</td>\n      <td>[0.6513966480446928, 0.6163841807909605, 0.5902857142857143, 0.5664739884393064]</td>\n      <td>1.000000</td>\n      <td>1.469622</td>\n      <td>1790</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.027900</td>\n      <td>0.130093</td>\n      <td>0.606422</td>\n      <td>[0.6533931575995513, 0.6176971072036301, 0.5909351692484223, 0.567034242600116]</td>\n      <td>1.000000</td>\n      <td>1.463875</td>\n      <td>1783</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.030400</td>\n      <td>0.130269</td>\n      <td>0.604010</td>\n      <td>[0.6508379888268156, 0.6152542372881356, 0.5885714285714285, 0.5647398843930636]</td>\n      <td>1.000000</td>\n      <td>1.469622</td>\n      <td>1790</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.024600</td>\n      <td>0.129679</td>\n      <td>0.605578</td>\n      <td>[0.6530498041410184, 0.6174306734578382, 0.589582140812822, 0.5657209033005212]</td>\n      <td>1.000000</td>\n      <td>1.467159</td>\n      <td>1787</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.029100</td>\n      <td>0.128872</td>\n      <td>0.607237</td>\n      <td>[0.6539753639417694, 0.6189127972819932, 0.5916380297823597, 0.5677867902665121]</td>\n      <td>1.000000</td>\n      <td>1.466338</td>\n      <td>1786</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.025800</td>\n      <td>0.128575</td>\n      <td>0.606891</td>\n      <td>[0.6536094012311136, 0.6185625353706847, 0.59129937034917, 0.567458019687319]</td>\n      <td>1.000000</td>\n      <td>1.467159</td>\n      <td>1787</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.026600</td>\n      <td>0.128648</td>\n      <td>0.604760</td>\n      <td>[0.6517607602012297, 0.6167326172979084, 0.5889079473985135, 0.5650665124349334]</td>\n      <td>1.000000</td>\n      <td>1.468801</td>\n      <td>1789</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.030300</td>\n      <td>0.128830</td>\n      <td>0.604353</td>\n      <td>[0.6526845637583892, 0.6159502262443439, 0.5881006864988558, 0.5642361111111112]</td>\n      <td>1.000000</td>\n      <td>1.467980</td>\n      <td>1788</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.027500</td>\n      <td>0.129105</td>\n      <td>0.605321</td>\n      <td>[0.6528787031861375, 0.6167326172979084, 0.5894797026872498, 0.5656448814343551]</td>\n      <td>1.000000</td>\n      <td>1.468801</td>\n      <td>1789</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.025600</td>\n      <td>0.129663</td>\n      <td>0.606701</td>\n      <td>[0.6543417366946779, 0.6181303116147309, 0.5908309455587393, 0.5669565217391305]</td>\n      <td>1.000000</td>\n      <td>1.465517</td>\n      <td>1785</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.026300</td>\n      <td>0.130284</td>\n      <td>0.608251</td>\n      <td>[0.6539966461710452, 0.6189937817976258, 0.5929102344196684, 0.5702718334297282]</td>\n      <td>1.000000</td>\n      <td>1.468801</td>\n      <td>1789</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.026300</td>\n      <td>0.130789</td>\n      <td>0.608944</td>\n      <td>[0.6547285954113039, 0.6196943972835314, 0.5935890097309674, 0.5709322524609148]</td>\n      <td>1.000000</td>\n      <td>1.467159</td>\n      <td>1787</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.027200</td>\n      <td>0.131093</td>\n      <td>0.608814</td>\n      <td>[0.6541689983212088, 0.6196943972835314, 0.5935890097309674, 0.5709322524609148]</td>\n      <td>1.000000</td>\n      <td>1.467159</td>\n      <td>1787</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.026600</td>\n      <td>0.131279</td>\n      <td>0.609477</td>\n      <td>[0.6543624161073825, 0.620475113122172, 0.5943935926773455, 0.5717592592592593]</td>\n      <td>1.000000</td>\n      <td>1.467980</td>\n      <td>1788</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.026400</td>\n      <td>0.131582</td>\n      <td>0.608251</td>\n      <td>[0.6539966461710452, 0.6189937817976258, 0.5929102344196684, 0.5702718334297282]</td>\n      <td>1.000000</td>\n      <td>1.468801</td>\n      <td>1789</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.025600</td>\n      <td>0.131751</td>\n      <td>0.609291</td>\n      <td>[0.6550951847704367, 0.6200453001132503, 0.5939289805269187, 0.5712630359212051]</td>\n      <td>1.000000</td>\n      <td>1.466338</td>\n      <td>1786</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.024700</td>\n      <td>0.131841</td>\n      <td>0.609261</td>\n      <td>[0.6545556176634991, 0.6201243640474845, 0.5940537449971413, 0.5714285714285714]</td>\n      <td>1.000000</td>\n      <td>1.468801</td>\n      <td>1789</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.024800</td>\n      <td>0.131947</td>\n      <td>0.608785</td>\n      <td>[0.6536312849162011, 0.619774011299435, 0.5937142857142857, 0.5710982658959538]</td>\n      <td>1.000000</td>\n      <td>1.469622</td>\n      <td>1790</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.026300</td>\n      <td>0.132039</td>\n      <td>0.608166</td>\n      <td>[0.6538031319910514, 0.619343891402715, 0.5926773455377574, 0.5700231481481481]</td>\n      <td>1.000000</td>\n      <td>1.467980</td>\n      <td>1788</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.024100</td>\n      <td>0.132031</td>\n      <td>0.607130</td>\n      <td>[0.6527079843662759, 0.6182947487295314, 0.5916619074814392, 0.5690352397458117]</td>\n      <td>1.000000</td>\n      <td>1.470443</td>\n      <td>1791</td>\n      <td>1218</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.025900</td>\n      <td>0.132094</td>\n      <td>0.608296</td>\n      <td>[0.6543624161073825, 0.619343891402715, 0.5926773455377574, 0.5700231481481481]</td>\n      <td>1.000000</td>\n      <td>1.467980</td>\n      <td>1788</td>\n      <td>1218</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"[0.52978515625, 0.4408284023668639, 0.3665338645418327, 0.3103621730382294]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.5320293398533007, 0.44493827160493826, 0.371072319201995, 0.3148614609571788]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.5329428989751098, 0.44701823558403153, 0.375311100049776, 0.32277526395173456]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.53369140625, 0.4521696252465483, 0.3844621513944223, 0.3335010060362173]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.5419921875, 0.46794871794871795, 0.40587649402390436, 0.36016096579476864]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.54296875, 0.46893491124260356, 0.4108565737051793, 0.3672032193158954]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.5464774951076321, 0.4772727272727273, 0.4216566866267465, 0.3800403225806452]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.5441678867740362, 0.4785608674223756, 0.4275759084121453, 0.3891402714932127]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.5657229524772498, 0.5056179775280899, 0.45923632610939114, 0.42231491136600624]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.578076525336091, 0.5156739811912225, 0.4677930306230201, 0.43062966915688367]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.5861522198731501, 0.5181623931623932, 0.4703023758099352, 0.4306768558951965]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.5879556259904913, 0.5242925787506674, 0.4749055585536967, 0.4348063284233497]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.5858111632759521, 0.5282024248813917, 0.48161960575386253, 0.4437264404954227]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.5871369294605809, 0.5319706498951782, 0.4894067796610169, 0.4544967880085653]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.5876556016597511, 0.529874213836478, 0.4851694915254237, 0.45021413276231265]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.5939614783966684, 0.5444502893214098, 0.5077086656034024, 0.47662547017732404]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.5928050052137643, 0.5442571127502634, 0.5111821086261981, 0.48008611410118407]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6034755134281201, 0.5508249068653539, 0.5099515868746638, 0.47743338771071236]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6072186836518046, 0.5547210300429185, 0.5151843817787418, 0.4830043859649123]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.615301724137931, 0.5615468409586056, 0.5209251101321586, 0.48775055679287305]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6245231607629428, 0.5735537190082645, 0.5337047353760446, 0.5025352112676056]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6325068870523416, 0.5844011142061282, 0.5442253521126761, 0.5122507122507123]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6338495575221239, 0.5850111856823266, 0.5463800904977375, 0.5137299771167048]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6387959866220736, 0.589064261555806, 0.5496009122006842, 0.5161476355247981]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.64501679731243, 0.6047565118912798, 0.5715922107674685, 0.5417149478563151]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6490438695163104, 0.6092150170648464, 0.5771001150747986, 0.5471478463329453]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.652542372881356, 0.616, 0.5878612716763005, 0.5584795321637427]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6630434782608695, 0.6255787037037037, 0.5936768149882904, 0.5633886255924171]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6622781911848884, 0.6247828604516502, 0.5934387814879907, 0.5649081209247184]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6643638457109959, 0.6266744321490972, 0.5928108426635239, 0.5641025641025641]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6660929432013769, 0.6297156123041208, 0.5977686435701702, 0.5710041592394534]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.667621776504298, 0.6347826086956522, 0.6058651026392962, 0.5780415430267062]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6710677382319173, 0.6387921022067363, 0.6110458284371327, 0.5868014268727705]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6733870967741935, 0.6375291375291375, 0.6055424528301887, 0.5787589498806682]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6737875288683602, 0.639018691588785, 0.607565011820331, 0.5801435406698564]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6733755031627372, 0.6410703897614892, 0.6121247792819305, 0.5872543180464562]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6710601719197707, 0.6359420289855072, 0.6052785923753665, 0.5798219584569733]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6697142857142857, 0.6341040462427746, 0.6035087719298246, 0.5781065088757397]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6670461013090495, 0.6332757628094415, 0.6051252184041933, 0.5804360636417207]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6662876634451392, 0.6325474410580794, 0.6050029086678301, 0.5809299587992937]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6685681688533942, 0.634737449509521, 0.6071220081727963, 0.5829887773183697]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6712485681557846, 0.6373117033603708, 0.6107854630715123, 0.5889679715302492]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6706758304696449, 0.6384704519119351, 0.611957796014068, 0.5889679715302492]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6714449541284404, 0.6380510440835266, 0.6109154929577465, 0.5878859857482185]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6735751295336787, 0.6371578334304019, 0.6087212728344137, 0.5849731663685152]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6760969976905312, 0.6401869158878505, 0.6122931442080378, 0.5891148325358851]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6731875719217492, 0.6367869615832363, 0.6089517078916372, 0.5852205005959475]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6697247706422018, 0.632830626450116, 0.6050469483568075, 0.581353919239905]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6636363636363637, 0.6275862068965518, 0.6, 0.5764705882352941]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6602600339174675, 0.6214979988564894, 0.5928282244071718, 0.569338794616735]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6617563739376771, 0.6217765042979942, 0.592463768115942, 0.5683284457478006]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6619479048697622, 0.6254295532646048, 0.5973348783314021, 0.5738569753810082]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6610073571024335, 0.6250715512306811, 0.5975680370584829, 0.5741066198008201]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6604519774011299, 0.624, 0.5953757225433526, 0.5719298245614035]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6578354002254791, 0.6197263397947549, 0.591118800461361, 0.5676779463243874]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6521008403361345, 0.6141643059490085, 0.5845272206303725, 0.56]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6510067114093959, 0.6153846153846154, 0.5881006864988558, 0.5642361111111112]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6504745951982133, 0.616600790513834, 0.5905197030268418, 0.5673021374927787]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6497206703910614, 0.6141242937853107, 0.5874285714285714, 0.5630057803468208]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6513966480446928, 0.6163841807909605, 0.5902857142857143, 0.5664739884393064]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6533931575995513, 0.6176971072036301, 0.5909351692484223, 0.567034242600116]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6508379888268156, 0.6152542372881356, 0.5885714285714285, 0.5647398843930636]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6530498041410184, 0.6174306734578382, 0.589582140812822, 0.5657209033005212]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6539753639417694, 0.6189127972819932, 0.5916380297823597, 0.5677867902665121]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6536094012311136, 0.6185625353706847, 0.59129937034917, 0.567458019687319]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6517607602012297, 0.6167326172979084, 0.5889079473985135, 0.5650665124349334]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6526845637583892, 0.6159502262443439, 0.5881006864988558, 0.5642361111111112]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6528787031861375, 0.6167326172979084, 0.5894797026872498, 0.5656448814343551]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6543417366946779, 0.6181303116147309, 0.5908309455587393, 0.5669565217391305]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6539966461710452, 0.6189937817976258, 0.5929102344196684, 0.5702718334297282]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6547285954113039, 0.6196943972835314, 0.5935890097309674, 0.5709322524609148]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6541689983212088, 0.6196943972835314, 0.5935890097309674, 0.5709322524609148]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6543624161073825, 0.620475113122172, 0.5943935926773455, 0.5717592592592593]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6539966461710452, 0.6189937817976258, 0.5929102344196684, 0.5702718334297282]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6550951847704367, 0.6200453001132503, 0.5939289805269187, 0.5712630359212051]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6545556176634991, 0.6201243640474845, 0.5940537449971413, 0.5714285714285714]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6536312849162011, 0.619774011299435, 0.5937142857142857, 0.5710982658959538]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6538031319910514, 0.619343891402715, 0.5926773455377574, 0.5700231481481481]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6527079843662759, 0.6182947487295314, 0.5916619074814392, 0.5690352397458117]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6543624161073825, 0.619343891402715, 0.5926773455377574, 0.5700231481481481]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"}]},{"cell_type":"code","source":"# alpaca_prompt = Copied from above\nhypotheses_list=[]\nreference_list=[]\nfor i in range(len(val_dataset)):\n  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n  inputs = tokenizer(\n  [\n      alpaca_prompt.format(\n          df['Schema'][66+i], # instruction\n          df['Question'][66+i], # input\n          \"\", # output - leave this blank for generation!\n      )\n  ], return_tensors = \"pt\").to(\"cuda\")\n\n\n  outputs = model.generate(**inputs, max_new_tokens = 500, use_cache = False)\n  x=tokenizer.batch_decode(outputs)[0]\n  resp_index =x.find('### Response:')\n\n  hypotheses_list.append(tokenizer.batch_decode(outputs)[0][resp_index+len(\"### Response:\"):-len(\"<|endoftext|>\")])\n  reference_list.append(df['Answer'][66+i])","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:21:54.374689Z","iopub.execute_input":"2024-06-22T22:21:54.375073Z","iopub.status.idle":"2024-06-22T22:23:24.062103Z","shell.execute_reply.started":"2024-06-22T22:21:54.375043Z","shell.execute_reply":"2024-06-22T22:23:24.060874Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\ndef calculate_bleu(hypotheses_list, reference_list):\n    # Convert references_list to a list of lists of references (required format for NLTK's corpus_bleu)\n    references = [[ref.split()] for ref in reference_list]\n\n    # Convert hypotheses_list to a list of lists of hypotheses (required format for NLTK's corpus_bleu)\n    hypotheses = [hyp.split() for hyp in hypotheses_list]\n\n    # Compute BLEU scores\n    bleu_scores = corpus_bleu(references, hypotheses)\n\n    return bleu_scores\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:02:20.320009Z","iopub.execute_input":"2024-06-22T22:02:20.320408Z","iopub.status.idle":"2024-06-22T22:02:20.931666Z","shell.execute_reply.started":"2024-06-22T22:02:20.320375Z","shell.execute_reply":"2024-06-22T22:02:20.930443Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"hypotheses_list","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:02:04.732780Z","iopub.execute_input":"2024-06-22T22:02:04.733171Z","iopub.status.idle":"2024-06-22T22:02:04.741633Z","shell.execute_reply.started":"2024-06-22T22:02:04.733138Z","shell.execute_reply":"2024-06-22T22:02:04.740563Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"['\\nMATCH (invoice:Invoice)-[:HAS_PRODUCT]->(product:ProductDescription)-[:HAS_UNIT_PRICE]->(unit_price:UnitPrice)\\nOPTIONAL MATCH (invoice)-[:HAS_TOTAL]->(total:Total)\\nRETURN invoice.name AS InvoiceID, total.name AS TotalAmo',\n '\\nMATCH (i:Invoice)-[:HAS_PRODUCT]->(p:ProductDescription)-[:HAS_TOTAL]->(t:Total)\\nWITH i, AVG(toFloat(p.name)) AS AvgProductTotal, COUNT(i) AS InvoiceCount\\nWHERE InvoiceCount > 1 AND AvgProductTotal > AVG(TOFLOAT(total))\\nRETURN i.name AS InvoiceName, AvgProductTotal AS AverageProductTotal, InvoiceCo',\n '\\nMATCH (i:Invoice)-[:HAS_PRODUCT]->(p:ProductDescription)-[:HAS_QUANTITY]->(q:Quantity)\\nOPTIONAL MATCH (i)-[:HAS_TOTAL]->(t:Total)\\nRETURN i.name AS InvoiceName, p.name AS ProductDescription, q.name AS Quantity, TOFloat(t.name) WITHIN GROUP (ORDER BY TOFloat(t.name) DESC) AS Total\\nWHERE TOFloat(TOFloat(total)) >',\n \"\\nMATCH (invoice:Invoice {name: 'INV67890'})-[:HAS_PRODUCT]->(product:ProductDescription)\\nRETURN product.name AS ProductDescription, SUM(TOFLOAT(quantity:Quantity.name)) AS TotalQuantity\\nWHERE invoice.name = 'INV678\",\n \"\\nMATCH (invoice:Invoice {name: 'INV10101'})-[:HAS_PRODUCT]->(product:ProductDescription)\\nMATCH (product)-[:HAS_TOTAL]->(total:Total)\\nRETURN product.name AS ProductDescription, \\n       SUM(toFloat(total.name)) AS TotalQuantity\\nINVOICE INV10\",\n '\\nMATCH (seller:Seller)<-[:HAS_SELLER]-(invoice:Invoice)-[:HAS_TOTAL_AMOUNT]->(total:TotalAmount)\\nRETURN DISTINCT seller.name, SUM(total.name) as total_revenue\\nORDER BY total_revenue DESC\\nLIMI',\n '\\nMATCH (i:Invoice)-[:HAS_PRODUCT]->(p:ProductDescription)\\nWITH i, COUNT(p.name) AS ProductCount\\nORDER BY ProductCount DESC\\nLIMIT 1\\nRETURN i.name AS InvoiceName, ProductCo',\n '\\nMATCH (i:Invoice)-[:HAS_PRODUCT]->(p:ProductDescription)\\nWITH i, COUNT(p) AS ProductCount\\nWHERE ProductCount = 1\\nRETURN i.name AS InvoiceName, ProductCo',\n \"\\nMATCH (invoice:Invoice {name: 'INV12345'})-[:HAS_SELLER]->(seller:Seller)\\nRETURN seller.n\",\n '\\nMATCH (i:Invoice)-[:HAS_CLIENT]->(c:Client), \\n      (i-has-many-criteria)-[:HAS_CLIENT]->(c)\\nRETURN i.name AS InvoiceName, c.name AS ClientN',\n '\\nMATCH (i:Invoice)-[:HAS_PRODUCT]->(p:ProductDescription)-[:HAS_QUANTITY]->(q:Quantity)\\nRETURN i.name AS InvoiceName, p.name AS ProductDescription, q.name AS Quant',\n \"\\nMATCH (invoice:Invoice {name: 'INV12345'})-[:HAS_PRODUCT]->(product:ProductDescription)\\nRETURN DISTINCT TO_FLOAT(product.name) AS ProductDescript\",\n '\\nMATCH (client:Client)<-[:HAS_CLIENT]-(invoice:Invoice)\\nRETURN DISTINCT(client.name) AS ClientN',\n '\\nMATCH (client:Client)<-[:HAS_CLIENT]-(invoice:Invoice)-[:HAS_PRODUCT]->(product:ProductDescription)-[:HAS_UNIT_PRICE]->(unitPrice:UnitPrice)\\nWHERE TOINTEGER(product.name) > MAX(toINTEGER(unitPrice.name))\\nRETURN client.name, product.name as product_description, TOINTEGER(unitPrice.name) as total_pr',\n \"\\ncypher MATCH (seller:Seller {name: 'SellerABC'})<-[:HAS_SELLER]-(invoice:Invoice)-[:HAS_PRODUCT]->(product:ProductDescription)-[:HAS_QUANTITY]->(quantity:Quantity) WHERE invoiceINV:Invoice = 'INV54321' RETURN product.name AS ProductDescription, quantity.quant\",\n '\\nMATCH (i:Invoice)-[:HAS_CLIENT]->(c:Client), \\n      (i:Invoice)-[:HAS_SELLER]->(s:Seller)\\nRETURN i.name AS InvoiceName, c.name AS ClientName, s.name AS SellerN',\n '\\nMATCH (client:Client)<-[:HAS_CLIENT]-(invoice:Invoice)-[:HAS_TOTAL_AMOUNT]->(total:TotalAmount)\\nRETURN client.name, SUM(total.name) AS TotalInvoiceAmount\\nORDER BY TotalInvoiceAmount DESC\\nLIMI',\n \"\\ncypher MATCH (seller:Seller {name: 'SellerXYZ'})-[:HAS_INVOICE]->(invoice:Invoice)-[:HAS_TOTAL_AMOUNT]->(total:TotalAmount) WHERE invoice.date >= date('2024-06-01') AND invoice.date <= date('2024-06-30') RETURN seller.name, SUM(total.name) as total_amo\",\n '\\nMATCH (i:Invoice)-[:HAS_PRODUCT]->(p:ProductDescription)-[:HAS_QUANTITY]->(q:Quantity)\\nRETURN i.name AS InvoiceName, p.name AS ProductDescription, q.name AS Quant',\n \"\\nMATCH (invoice:Invoice {name: 'INV55555'})-[:HAS_PRODUCT]->(product:ProductDescription)\\nRETURN DISTINCT product.name AS ProductDescript\"]"},"metadata":{}}]},{"cell_type":"code","source":"results = bleu.compute(predictions=hypotheses_list,references=reference_list)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:23:30.693511Z","iopub.execute_input":"2024-06-22T22:23:30.693931Z","iopub.status.idle":"2024-06-22T22:23:30.724949Z","shell.execute_reply.started":"2024-06-22T22:23:30.693899Z","shell.execute_reply":"2024-06-22T22:23:30.723921Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:23:31.019790Z","iopub.execute_input":"2024-06-22T22:23:31.020201Z","iopub.status.idle":"2024-06-22T22:23:31.028856Z","shell.execute_reply.started":"2024-06-22T22:23:31.020169Z","shell.execute_reply":"2024-06-22T22:23:31.027754Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"{'bleu': 0.7387941130694241,\n 'precisions': [0.8940639269406393,\n  0.8437209302325581,\n  0.8037914691943128,\n  0.770048309178744],\n 'brevity_penalty': 0.8937503734540803,\n 'length_ratio': 0.8990147783251231,\n 'translation_length': 1095,\n 'reference_length': 1218}"},"metadata":{}}]},{"cell_type":"code","source":"bleu_scores = calculate_bleu(hypotheses_list, reference_list)\nprint(\"BLEU Scores:\", bleu_scores)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:23:40.645777Z","iopub.execute_input":"2024-06-22T22:23:40.646159Z","iopub.status.idle":"2024-06-22T22:23:40.658711Z","shell.execute_reply.started":"2024-06-22T22:23:40.646128Z","shell.execute_reply":"2024-06-22T22:23:40.657528Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"BLEU Scores: 0.4637061896572921\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:03:24.012053Z","iopub.execute_input":"2024-06-22T22:03:24.012439Z","iopub.status.idle":"2024-06-22T22:03:40.417145Z","shell.execute_reply.started":"2024-06-22T22:03:24.012407Z","shell.execute_reply":"2024-06-22T22:03:40.415921Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=4d007303ba0083fec790569251443e9d9e6420738c201a3234e8e026b9353fca\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"rouge = evaluate.load('rouge')\n\n\nresults = rouge.compute(predictions=hypotheses_list,references=reference_list)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:23:46.232423Z","iopub.execute_input":"2024-06-22T22:23:46.232832Z","iopub.status.idle":"2024-06-22T22:23:46.775955Z","shell.execute_reply.started":"2024-06-22T22:23:46.232802Z","shell.execute_reply":"2024-06-22T22:23:46.774922Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:23:48.527233Z","iopub.execute_input":"2024-06-22T22:23:48.527631Z","iopub.status.idle":"2024-06-22T22:23:48.534992Z","shell.execute_reply.started":"2024-06-22T22:23:48.527598Z","shell.execute_reply":"2024-06-22T22:23:48.533877Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"{'rouge1': 0.803438113697083,\n 'rouge2': 0.7187245805684787,\n 'rougeL': 0.7873416579539574,\n 'rougeLsum': 0.7986729776141843}"},"metadata":{}}]},{"cell_type":"code","source":"for hyp,ref in zip(hypotheses_list,reference_list):\n    print(hyp)\n    print(\"\\n\")\n    print(ref)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\") # Local saving\ntokenizer.save_pretrained(\"lora_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(\"alterf/cypher2text\", token = \"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\") # Online saving\ntokenizer.push_to_hub(\"alterf/cypher2text\", token = \"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\") # Online saving","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}