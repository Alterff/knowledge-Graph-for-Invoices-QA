{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8534316,"sourceType":"datasetVersion","datasetId":5097499},{"sourceId":8534337,"sourceType":"datasetVersion","datasetId":5097515},{"sourceId":8551319,"sourceType":"datasetVersion","datasetId":5110007},{"sourceId":8694656,"sourceType":"datasetVersion","datasetId":5214082},{"sourceId":8694962,"sourceType":"datasetVersion","datasetId":5214262}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nmajor_version, minor_version = torch.cuda.get_device_capability()\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\nif major_version >= 8:\n    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\nelse:\n    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n    !pip install --no-deps xformers trl peft accelerate bitsandbytes\npass\n!pip install triton transformers\n!pip install -U datasets\n!pip install --pre -U xformers ##### this take some time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****Note** Restart the Kernal after package installation**","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HFToken\")\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-15T15:05:33.617227Z","iopub.execute_input":"2024-06-15T15:05:33.617592Z","iopub.status.idle":"2024-06-15T15:05:33.881307Z","shell.execute_reply.started":"2024-06-15T15:05:33.617561Z","shell.execute_reply":"2024-06-15T15:05:33.880365Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nfrom IPython.display import display_markdown\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/Qwen2-0.5B\",  \n]  #### loadin llama 3 model in 4 bit to fine tune","metadata":{"execution":{"iopub.status.busy":"2024-06-15T15:05:35.643223Z","iopub.execute_input":"2024-06-15T15:05:35.643639Z","iopub.status.idle":"2024-06-15T15:05:52.034000Z","shell.execute_reply.started":"2024-06-15T15:05:35.643599Z","shell.execute_reply":"2024-06-15T15:05:52.032986Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2024-06-15 15:05:43.599722: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-15 15:05:43.599817: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-15 15:05:43.709792: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Qwen2-0.5B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = False,\n    token = secret_value_0\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:09:26.669509Z","iopub.execute_input":"2024-06-15T17:09:26.669931Z","iopub.status.idle":"2024-06-15T17:09:34.092424Z","shell.execute_reply.started":"2024-06-15T17:09:26.669898Z","shell.execute_reply":"2024-06-15T17:09:34.091311Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"==((====))==  Unsloth: Fast Qwen2 patching release 2024.6\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. Xformers = 0.0.27.dev792. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:10:54.988058Z","iopub.execute_input":"2024-06-15T17:10:54.988779Z","iopub.status.idle":"2024-06-15T17:10:59.945161Z","shell.execute_reply.started":"2024-06-15T17:10:54.988745Z","shell.execute_reply":"2024-06-15T17:10:59.943701Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom datasets import load_dataset\nimport numpy as np\nseed = 42\nnp.random.seed(seed)\nimport pandas as pd\ndf=pd.read_excel('/kaggle/input/bnbbvc/text2cypher.xlsx')\n\n\ndef format_schema(schema):\n    return f\"\"\"You are an expert in querying Neo4j graph databases using the Cypher query language. You are provided with a specific graph schema and your task is to generate accurate and syntactically correct Cypher queries based on questions about this graph. The graph schema is as follows <schema>:\\n{schema}\\n</schema>\"\"\"\n\n# Apply the formatting function to the 'new_schema' column\ndf['Schema'] = df['Schema'].apply(lambda x: format_schema(x))\n\nfrom datasets import Dataset, DatasetDict\ndf = df.sample(frac=1).reset_index(drop=True)\ntrain_df = df.iloc[:30]\nval_df = df.iloc[30:]\ntrain_df\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ndataset = DatasetDict()\n# Create a Hugging Face Dataset from the dictionary\ndataset['train'] = train_dataset\ndataset['val'] = val_dataset\ndataset_subset = dataset[\"train\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:10:59.947820Z","iopub.execute_input":"2024-06-15T17:10:59.948993Z","iopub.status.idle":"2024-06-15T17:11:00.012881Z","shell.execute_reply.started":"2024-06-15T17:10:59.948955Z","shell.execute_reply":"2024-06-15T17:11:00.011736Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides the Question. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    \n    instructions = examples[\"Schema\"]\n    inputs       = examples[\"Question\"]\n    outputs      = examples[\"Answer\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\n\ndataset = dataset_subset.map(formatting_prompts_func, batched = True,)\nval_dataset=val_dataset.map(formatting_prompts_func,batched=True,)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:11:04.717339Z","iopub.execute_input":"2024-06-15T17:11:04.717738Z","iopub.status.idle":"2024-06-15T17:11:04.770402Z","shell.execute_reply.started":"2024-06-15T17:11:04.717707Z","shell.execute_reply":"2024-06-15T17:11:04.769416Z"},"trusted":true},"execution_count":86,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fddc34a19c74d618e06f383663176c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a4ef5a789244088b5a071455a2457fd"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_metric\nmetric = load_metric('bleu')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T15:10:15.535977Z","iopub.execute_input":"2024-06-15T15:10:15.536808Z","iopub.status.idle":"2024-06-15T15:10:18.399762Z","shell.execute_reply.started":"2024-06-15T15:10:15.536773Z","shell.execute_reply":"2024-06-15T15:10:18.398972Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_169/576045966.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n  metric = load_metric('bleu')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90d9a24e71af475d86a729f936c63c72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66d33d75daf14d9298c570db915c43fe"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for bleu contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bleu.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"}]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=-1)\n    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    for i,pred in enumerate(decoded_preds):\n        x=pred.find(\"### Response\")\n        decoded_preds[i]=pred[x+len(\"### Response \"):]\n    for i,lab in enumerate(decoded_labels):\n        x=lab.find(\"### Response\")\n        decoded_labels[i]=lab[x+len(\"### Response \"):]\n\n       \n    return bleu.compute(predictions=decoded_preds, references=decoded_labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:11:29.062179Z","iopub.execute_input":"2024-06-15T17:11:29.063049Z","iopub.status.idle":"2024-06-15T17:11:29.073268Z","shell.execute_reply.started":"2024-06-15T17:11:29.063004Z","shell.execute_reply":"2024-06-15T17:11:29.072090Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    eval_dataset = val_dataset,\n    compute_metrics=compute_metrics,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        auto_find_batch_size=True,\n        gradient_accumulation_steps = 2,\n        warmup_steps = 18,\n        max_steps = 70,\n        learning_rate = 2e-4,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        load_best_model_at_end = True,\n        eval_accumulation_steps = 1,\n        evaluation_strategy = \"steps\",\n        eval_steps = 1,\n    ),\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:12:32.724897Z","iopub.execute_input":"2024-06-15T17:12:32.725830Z","iopub.status.idle":"2024-06-15T17:12:35.240062Z","shell.execute_reply.started":"2024-06-15T17:12:32.725796Z","shell.execute_reply":"2024-06-15T17:12:35.238987Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a315b5c54ef4a33894231002cfbc3c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba890cba970e48b3a2fd1ffa81cb2fc0"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:12:37.786656Z","iopub.execute_input":"2024-06-15T17:12:37.787676Z","iopub.status.idle":"2024-06-15T17:17:55.451491Z","shell.execute_reply.started":"2024-06-15T17:12:37.787636Z","shell.execute_reply":"2024-06-15T17:17:55.450411Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 30 | Num Epochs = 35\nO^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 2\n\\        /    Total batch size = 16 | Total steps = 70\n \"-____-\"     Number of trainable parameters = 8,798,208\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [70/70 05:11, Epoch 35/35]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Precisions</th>\n      <th>Brevity Penalty</th>\n      <th>Length Ratio</th>\n      <th>Translation Length</th>\n      <th>Reference Length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.716300</td>\n      <td>1.729857</td>\n      <td>0.101868</td>\n      <td>[0.1320754716981132, 0.11054370681886452, 0.09279903585417294, 0.0794802055001511]</td>\n      <td>1.000000</td>\n      <td>6.731855</td>\n      <td>3339</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.729500</td>\n      <td>1.723246</td>\n      <td>0.102878</td>\n      <td>[0.13263473053892216, 0.1114114114114114, 0.09397590361445783, 0.08066465256797584]</td>\n      <td>1.000000</td>\n      <td>6.733871</td>\n      <td>3340</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.724800</td>\n      <td>1.692798</td>\n      <td>0.103122</td>\n      <td>[0.13285457809694792, 0.11164465786314526, 0.09422034918723661, 0.08091787439613526]</td>\n      <td>1.000000</td>\n      <td>6.737903</td>\n      <td>3342</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.675800</td>\n      <td>1.639080</td>\n      <td>0.103555</td>\n      <td>[0.13345302214242968, 0.11224489795918367, 0.09452137266706803, 0.08121980676328502]</td>\n      <td>1.000000</td>\n      <td>6.737903</td>\n      <td>3342</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.611400</td>\n      <td>1.575575</td>\n      <td>0.103488</td>\n      <td>[0.13252297657871331, 0.11210228962236099, 0.0945422010140173, 0.08166317678731679]</td>\n      <td>1.000000</td>\n      <td>6.800403</td>\n      <td>3373</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.588100</td>\n      <td>1.503436</td>\n      <td>0.232495</td>\n      <td>[0.29791666666666666, 0.25384615384615383, 0.2119718309859155, 0.1822695035460993]</td>\n      <td>1.000000</td>\n      <td>2.903226</td>\n      <td>1440</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.494500</td>\n      <td>1.422754</td>\n      <td>0.527066</td>\n      <td>[0.6824877250409165, 0.5740432612312812, 0.47884940778341795, 0.4113597246127367]</td>\n      <td>1.000000</td>\n      <td>1.231855</td>\n      <td>611</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.408800</td>\n      <td>1.334209</td>\n      <td>0.524390</td>\n      <td>[0.6818923327895595, 0.5688225538971807, 0.47554806070826305, 0.4099485420240137]</td>\n      <td>1.000000</td>\n      <td>1.235887</td>\n      <td>613</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.317200</td>\n      <td>1.239729</td>\n      <td>0.536348</td>\n      <td>[0.6833602584814217, 0.5763546798029556, 0.48914858096828046, 0.4295415959252971]</td>\n      <td>1.000000</td>\n      <td>1.247984</td>\n      <td>619</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.227500</td>\n      <td>1.138246</td>\n      <td>0.549408</td>\n      <td>[0.6913183279742765, 0.5849673202614379, 0.5033222591362126, 0.44763513513513514]</td>\n      <td>1.000000</td>\n      <td>1.254032</td>\n      <td>622</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.118900</td>\n      <td>1.028480</td>\n      <td>0.549606</td>\n      <td>[0.6897106109324759, 0.5849673202614379, 0.5033222591362126, 0.44932432432432434]</td>\n      <td>1.000000</td>\n      <td>1.254032</td>\n      <td>622</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.017000</td>\n      <td>0.907617</td>\n      <td>0.566734</td>\n      <td>[0.6978998384491115, 0.5993431855500821, 0.5225375626043406, 0.47198641765704585]</td>\n      <td>1.000000</td>\n      <td>1.247984</td>\n      <td>619</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.888400</td>\n      <td>0.775363</td>\n      <td>0.581498</td>\n      <td>[0.7050243111831442, 0.6128500823723229, 0.5393634840871022, 0.4906303236797274]</td>\n      <td>1.000000</td>\n      <td>1.243952</td>\n      <td>617</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.756400</td>\n      <td>0.662260</td>\n      <td>0.581823</td>\n      <td>[0.702572347266881, 0.6127450980392157, 0.5415282392026578, 0.49155405405405406]</td>\n      <td>1.000000</td>\n      <td>1.254032</td>\n      <td>622</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.644200</td>\n      <td>0.550452</td>\n      <td>0.579127</td>\n      <td>[0.6936507936507936, 0.6096774193548387, 0.5409836065573771, 0.49166666666666664]</td>\n      <td>1.000000</td>\n      <td>1.270161</td>\n      <td>630</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.515400</td>\n      <td>0.460022</td>\n      <td>0.561035</td>\n      <td>[0.6770670826833073, 0.5911251980982567, 0.5233494363929146, 0.47299509001636664]</td>\n      <td>1.000000</td>\n      <td>1.292339</td>\n      <td>641</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.420300</td>\n      <td>0.367669</td>\n      <td>0.551780</td>\n      <td>[0.6636085626911316, 0.5807453416149069, 0.5157728706624606, 0.46634615384615385]</td>\n      <td>1.000000</td>\n      <td>1.318548</td>\n      <td>654</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.350600</td>\n      <td>0.302996</td>\n      <td>0.533657</td>\n      <td>[0.6444444444444445, 0.562406015037594, 0.49770992366412214, 0.4496124031007752]</td>\n      <td>1.000000</td>\n      <td>1.360887</td>\n      <td>675</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.268300</td>\n      <td>0.252283</td>\n      <td>0.519721</td>\n      <td>[0.6311953352769679, 0.5502958579881657, 0.48348348348348347, 0.4344512195121951]</td>\n      <td>1.000000</td>\n      <td>1.383065</td>\n      <td>686</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.212000</td>\n      <td>0.219980</td>\n      <td>0.518734</td>\n      <td>[0.6277372262773723, 0.5511111111111111, 0.48270676691729325, 0.433587786259542]</td>\n      <td>1.000000</td>\n      <td>1.381048</td>\n      <td>685</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.187100</td>\n      <td>0.200992</td>\n      <td>0.521990</td>\n      <td>[0.627906976744186, 0.551622418879056, 0.4880239520958084, 0.439209726443769]</td>\n      <td>1.000000</td>\n      <td>1.387097</td>\n      <td>688</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.136900</td>\n      <td>0.186847</td>\n      <td>0.526687</td>\n      <td>[0.6334310850439883, 0.5565476190476191, 0.49244712990936557, 0.4432515337423313]</td>\n      <td>1.000000</td>\n      <td>1.375000</td>\n      <td>682</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.134000</td>\n      <td>0.179800</td>\n      <td>0.531649</td>\n      <td>[0.636896046852123, 0.5616641901931649, 0.497737556561086, 0.44869831546707506]</td>\n      <td>1.000000</td>\n      <td>1.377016</td>\n      <td>683</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.115900</td>\n      <td>0.174392</td>\n      <td>0.528809</td>\n      <td>[0.6329479768786127, 0.5557184750733137, 0.4955357142857143, 0.4486404833836858]</td>\n      <td>1.000000</td>\n      <td>1.395161</td>\n      <td>692</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.118400</td>\n      <td>0.170146</td>\n      <td>0.534531</td>\n      <td>[0.6345323741007194, 0.5605839416058395, 0.5037037037037037, 0.4556390977443609]</td>\n      <td>1.000000</td>\n      <td>1.401210</td>\n      <td>695</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.096900</td>\n      <td>0.167143</td>\n      <td>0.533416</td>\n      <td>[0.6278409090909091, 0.5576368876080692, 0.5043859649122807, 0.45845697329376855]</td>\n      <td>1.000000</td>\n      <td>1.419355</td>\n      <td>704</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.105000</td>\n      <td>0.164876</td>\n      <td>0.535235</td>\n      <td>[0.6227208976157083, 0.5590327169274538, 0.5079365079365079, 0.46412884333821375]</td>\n      <td>1.000000</td>\n      <td>1.437500</td>\n      <td>713</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.090700</td>\n      <td>0.163124</td>\n      <td>0.537074</td>\n      <td>[0.6215083798882681, 0.5609065155807366, 0.5100574712643678, 0.4679300291545189]</td>\n      <td>1.000000</td>\n      <td>1.443548</td>\n      <td>716</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.089000</td>\n      <td>0.161466</td>\n      <td>0.534717</td>\n      <td>[0.6174033149171271, 0.5560224089635855, 0.5085227272727273, 0.46829971181556196]</td>\n      <td>1.000000</td>\n      <td>1.459677</td>\n      <td>724</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.089700</td>\n      <td>0.160295</td>\n      <td>0.533254</td>\n      <td>[0.6151724137931035, 0.5552447552447553, 0.5078014184397163, 0.46618705035971225]</td>\n      <td>1.000000</td>\n      <td>1.461694</td>\n      <td>725</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.080300</td>\n      <td>0.159442</td>\n      <td>0.531646</td>\n      <td>[0.616551724137931, 0.5538461538461539, 0.5049645390070922, 0.4633093525179856]</td>\n      <td>1.000000</td>\n      <td>1.461694</td>\n      <td>725</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.077200</td>\n      <td>0.157444</td>\n      <td>0.531805</td>\n      <td>[0.6175243393602226, 0.5557122708039492, 0.5050071530758226, 0.46153846153846156]</td>\n      <td>1.000000</td>\n      <td>1.449597</td>\n      <td>719</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.067300</td>\n      <td>0.155072</td>\n      <td>0.534674</td>\n      <td>[0.6206415620641562, 0.5601131541725601, 0.5078909612625538, 0.462882096069869]</td>\n      <td>1.000000</td>\n      <td>1.445565</td>\n      <td>717</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.073500</td>\n      <td>0.153168</td>\n      <td>0.538951</td>\n      <td>[0.623249299719888, 0.5639204545454546, 0.5115273775216138, 0.4692982456140351]</td>\n      <td>1.000000</td>\n      <td>1.439516</td>\n      <td>714</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.067200</td>\n      <td>0.151977</td>\n      <td>0.535212</td>\n      <td>[0.6192468619246861, 0.5601131541725601, 0.5078909612625538, 0.4657933042212518]</td>\n      <td>1.000000</td>\n      <td>1.445565</td>\n      <td>717</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.059200</td>\n      <td>0.151097</td>\n      <td>0.539761</td>\n      <td>[0.6215083798882681, 0.5637393767705382, 0.5129310344827587, 0.47230320699708456]</td>\n      <td>1.000000</td>\n      <td>1.443548</td>\n      <td>716</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.064500</td>\n      <td>0.150307</td>\n      <td>0.539761</td>\n      <td>[0.6215083798882681, 0.5637393767705382, 0.5129310344827587, 0.47230320699708456]</td>\n      <td>1.000000</td>\n      <td>1.443548</td>\n      <td>716</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.050400</td>\n      <td>0.150993</td>\n      <td>0.538151</td>\n      <td>[0.623249299719888, 0.5639204545454546, 0.5100864553314121, 0.4678362573099415]</td>\n      <td>1.000000</td>\n      <td>1.439516</td>\n      <td>714</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.053600</td>\n      <td>0.151411</td>\n      <td>0.544383</td>\n      <td>[0.6303116147308782, 0.5704022988505747, 0.5160349854227405, 0.47337278106508873]</td>\n      <td>1.000000</td>\n      <td>1.423387</td>\n      <td>706</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.051100</td>\n      <td>0.152548</td>\n      <td>0.556272</td>\n      <td>[0.6354609929078014, 0.5798561151079137, 0.5299270072992701, 0.49037037037037035]</td>\n      <td>1.000000</td>\n      <td>1.421371</td>\n      <td>705</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.048800</td>\n      <td>0.154095</td>\n      <td>0.558702</td>\n      <td>[0.6381766381766382, 0.5823699421965318, 0.532258064516129, 0.49255952380952384]</td>\n      <td>1.000000</td>\n      <td>1.415323</td>\n      <td>702</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.049200</td>\n      <td>0.154600</td>\n      <td>0.565780</td>\n      <td>[0.640625, 0.5878962536023055, 0.5409356725146199, 0.5029673590504451]</td>\n      <td>1.000000</td>\n      <td>1.419355</td>\n      <td>704</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.042200</td>\n      <td>0.155315</td>\n      <td>0.569917</td>\n      <td>[0.6452074391988555, 0.5921625544267054, 0.5449189985272459, 0.5067264573991032]</td>\n      <td>1.000000</td>\n      <td>1.409274</td>\n      <td>699</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.046100</td>\n      <td>0.156341</td>\n      <td>0.576292</td>\n      <td>[0.6494252873563219, 0.597667638483965, 0.5517751479289941, 0.515015015015015]</td>\n      <td>1.000000</td>\n      <td>1.403226</td>\n      <td>696</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.041600</td>\n      <td>0.158129</td>\n      <td>0.563346</td>\n      <td>[0.6428571428571429, 0.5855072463768116, 0.5367647058823529, 0.49850746268656715]</td>\n      <td>1.000000</td>\n      <td>1.411290</td>\n      <td>700</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.040700</td>\n      <td>0.159687</td>\n      <td>0.560730</td>\n      <td>[0.642346208869814, 0.5834542815674891, 0.5331369661266568, 0.4947683109118087]</td>\n      <td>1.000000</td>\n      <td>1.409274</td>\n      <td>699</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.037100</td>\n      <td>0.162326</td>\n      <td>0.562632</td>\n      <td>[0.6413199426111909, 0.5836972343522562, 0.5361890694239291, 0.4992503748125937]</td>\n      <td>1.000000</td>\n      <td>1.405242</td>\n      <td>697</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.037200</td>\n      <td>0.163798</td>\n      <td>0.565675</td>\n      <td>[0.6436781609195402, 0.5874635568513119, 0.5399408284023669, 0.5015015015015015]</td>\n      <td>1.000000</td>\n      <td>1.403226</td>\n      <td>696</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.032600</td>\n      <td>0.163964</td>\n      <td>0.559452</td>\n      <td>[0.6363636363636364, 0.5821325648414986, 0.533625730994152, 0.49554896142433236]</td>\n      <td>1.000000</td>\n      <td>1.419355</td>\n      <td>704</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.037200</td>\n      <td>0.164884</td>\n      <td>0.561081</td>\n      <td>[0.6381766381766382, 0.5838150289017341, 0.5351906158357771, 0.49702380952380953]</td>\n      <td>1.000000</td>\n      <td>1.415323</td>\n      <td>702</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.033700</td>\n      <td>0.166190</td>\n      <td>0.563035</td>\n      <td>[0.6381766381766382, 0.5852601156069365, 0.5381231671554252, 0.5]</td>\n      <td>1.000000</td>\n      <td>1.415323</td>\n      <td>702</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.031000</td>\n      <td>0.167218</td>\n      <td>0.561958</td>\n      <td>[0.634180790960452, 0.5830945558739254, 0.5377906976744186, 0.5014749262536873]</td>\n      <td>1.000000</td>\n      <td>1.427419</td>\n      <td>708</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.032500</td>\n      <td>0.167985</td>\n      <td>0.562577</td>\n      <td>[0.6338028169014085, 0.5828571428571429, 0.5391304347826087, 0.5029411764705882]</td>\n      <td>1.000000</td>\n      <td>1.431452</td>\n      <td>710</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.027500</td>\n      <td>0.169889</td>\n      <td>0.558557</td>\n      <td>[0.6293706293706294, 0.5787234042553191, 0.5352517985611511, 0.4992700729927007]</td>\n      <td>1.000000</td>\n      <td>1.441532</td>\n      <td>715</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.027400</td>\n      <td>0.172626</td>\n      <td>0.563950</td>\n      <td>[0.6286509040333796, 0.5825105782792666, 0.542203147353362, 0.5094339622641509]</td>\n      <td>1.000000</td>\n      <td>1.449597</td>\n      <td>719</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.029900</td>\n      <td>0.174873</td>\n      <td>0.566056</td>\n      <td>[0.6282940360610264, 0.5836849507735584, 0.5449358059914408, 0.5137481910274964]</td>\n      <td>1.000000</td>\n      <td>1.453629</td>\n      <td>721</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.028000</td>\n      <td>0.176952</td>\n      <td>0.562866</td>\n      <td>[0.6248275862068966, 0.5804195804195804, 0.5418439716312057, 0.5107913669064749]</td>\n      <td>1.000000</td>\n      <td>1.461694</td>\n      <td>725</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.025800</td>\n      <td>0.178472</td>\n      <td>0.562866</td>\n      <td>[0.6248275862068966, 0.5804195804195804, 0.5418439716312057, 0.5107913669064749]</td>\n      <td>1.000000</td>\n      <td>1.461694</td>\n      <td>725</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.027100</td>\n      <td>0.179627</td>\n      <td>0.563660</td>\n      <td>[0.6256906077348067, 0.5812324929971989, 0.5426136363636364, 0.5115273775216138]</td>\n      <td>1.000000</td>\n      <td>1.459677</td>\n      <td>724</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.023900</td>\n      <td>0.180891</td>\n      <td>0.562866</td>\n      <td>[0.6248275862068966, 0.5804195804195804, 0.5418439716312057, 0.5107913669064749]</td>\n      <td>1.000000</td>\n      <td>1.461694</td>\n      <td>725</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.023800</td>\n      <td>0.182723</td>\n      <td>0.565875</td>\n      <td>[0.627939142461964, 0.5834502103786816, 0.5448079658605974, 0.5137085137085137]</td>\n      <td>1.000000</td>\n      <td>1.457661</td>\n      <td>723</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.025300</td>\n      <td>0.184368</td>\n      <td>0.565077</td>\n      <td>[0.6270718232044199, 0.5826330532212886, 0.5440340909090909, 0.5129682997118156]</td>\n      <td>1.000000</td>\n      <td>1.459677</td>\n      <td>724</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.023400</td>\n      <td>0.186132</td>\n      <td>0.562695</td>\n      <td>[0.624484181568088, 0.5801952580195258, 0.5417256011315418, 0.5107604017216643]</td>\n      <td>1.000000</td>\n      <td>1.465726</td>\n      <td>727</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.023600</td>\n      <td>0.187748</td>\n      <td>0.564280</td>\n      <td>[0.6262068965517241, 0.5818181818181818, 0.5432624113475177, 0.5122302158273381]</td>\n      <td>1.000000</td>\n      <td>1.461694</td>\n      <td>725</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.023000</td>\n      <td>0.188887</td>\n      <td>0.567478</td>\n      <td>[0.6296809986130375, 0.5850914205344585, 0.5463623395149786, 0.5151953690303908]</td>\n      <td>1.000000</td>\n      <td>1.453629</td>\n      <td>721</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.022800</td>\n      <td>0.189934</td>\n      <td>0.558239</td>\n      <td>[0.6222527472527473, 0.5766016713091922, 0.536723163841808, 0.504297994269341]</td>\n      <td>1.000000</td>\n      <td>1.467742</td>\n      <td>728</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.021900</td>\n      <td>0.190423</td>\n      <td>0.560598</td>\n      <td>[0.6248275862068966, 0.579020979020979, 0.5390070921985816, 0.5064748201438849]</td>\n      <td>1.000000</td>\n      <td>1.461694</td>\n      <td>725</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.023000</td>\n      <td>0.191007</td>\n      <td>0.561389</td>\n      <td>[0.6256906077348067, 0.5798319327731093, 0.5397727272727273, 0.5072046109510087]</td>\n      <td>1.000000</td>\n      <td>1.459677</td>\n      <td>724</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.024200</td>\n      <td>0.191328</td>\n      <td>0.559810</td>\n      <td>[0.6239669421487604, 0.5782122905027933, 0.5382436260623229, 0.5057471264367817]</td>\n      <td>1.000000</td>\n      <td>1.463710</td>\n      <td>726</td>\n      <td>496</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.019600</td>\n      <td>0.191554</td>\n      <td>0.562978</td>\n      <td>[0.6274238227146814, 0.5814606741573034, 0.5413105413105413, 0.5086705202312138]</td>\n      <td>1.000000</td>\n      <td>1.455645</td>\n      <td>722</td>\n      <td>496</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"[0.1320754716981132, 0.11054370681886452, 0.09279903585417294, 0.0794802055001511]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.13263473053892216, 0.1114114114114114, 0.09397590361445783, 0.08066465256797584]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.13285457809694792, 0.11164465786314526, 0.09422034918723661, 0.08091787439613526]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.13345302214242968, 0.11224489795918367, 0.09452137266706803, 0.08121980676328502]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.13252297657871331, 0.11210228962236099, 0.0945422010140173, 0.08166317678731679]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.29791666666666666, 0.25384615384615383, 0.2119718309859155, 0.1822695035460993]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6824877250409165, 0.5740432612312812, 0.47884940778341795, 0.4113597246127367]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6818923327895595, 0.5688225538971807, 0.47554806070826305, 0.4099485420240137]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6833602584814217, 0.5763546798029556, 0.48914858096828046, 0.4295415959252971]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6913183279742765, 0.5849673202614379, 0.5033222591362126, 0.44763513513513514]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6897106109324759, 0.5849673202614379, 0.5033222591362126, 0.44932432432432434]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6978998384491115, 0.5993431855500821, 0.5225375626043406, 0.47198641765704585]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.7050243111831442, 0.6128500823723229, 0.5393634840871022, 0.4906303236797274]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.702572347266881, 0.6127450980392157, 0.5415282392026578, 0.49155405405405406]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6936507936507936, 0.6096774193548387, 0.5409836065573771, 0.49166666666666664]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6770670826833073, 0.5911251980982567, 0.5233494363929146, 0.47299509001636664]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6636085626911316, 0.5807453416149069, 0.5157728706624606, 0.46634615384615385]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6444444444444445, 0.562406015037594, 0.49770992366412214, 0.4496124031007752]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6311953352769679, 0.5502958579881657, 0.48348348348348347, 0.4344512195121951]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6277372262773723, 0.5511111111111111, 0.48270676691729325, 0.433587786259542]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.627906976744186, 0.551622418879056, 0.4880239520958084, 0.439209726443769]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6334310850439883, 0.5565476190476191, 0.49244712990936557, 0.4432515337423313]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.636896046852123, 0.5616641901931649, 0.497737556561086, 0.44869831546707506]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6329479768786127, 0.5557184750733137, 0.4955357142857143, 0.4486404833836858]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6345323741007194, 0.5605839416058395, 0.5037037037037037, 0.4556390977443609]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6278409090909091, 0.5576368876080692, 0.5043859649122807, 0.45845697329376855]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6227208976157083, 0.5590327169274538, 0.5079365079365079, 0.46412884333821375]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6215083798882681, 0.5609065155807366, 0.5100574712643678, 0.4679300291545189]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6174033149171271, 0.5560224089635855, 0.5085227272727273, 0.46829971181556196]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6151724137931035, 0.5552447552447553, 0.5078014184397163, 0.46618705035971225]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.616551724137931, 0.5538461538461539, 0.5049645390070922, 0.4633093525179856]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6175243393602226, 0.5557122708039492, 0.5050071530758226, 0.46153846153846156]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6206415620641562, 0.5601131541725601, 0.5078909612625538, 0.462882096069869]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.623249299719888, 0.5639204545454546, 0.5115273775216138, 0.4692982456140351]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6192468619246861, 0.5601131541725601, 0.5078909612625538, 0.4657933042212518]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6215083798882681, 0.5637393767705382, 0.5129310344827587, 0.47230320699708456]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6215083798882681, 0.5637393767705382, 0.5129310344827587, 0.47230320699708456]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.623249299719888, 0.5639204545454546, 0.5100864553314121, 0.4678362573099415]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6303116147308782, 0.5704022988505747, 0.5160349854227405, 0.47337278106508873]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6354609929078014, 0.5798561151079137, 0.5299270072992701, 0.49037037037037035]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6381766381766382, 0.5823699421965318, 0.532258064516129, 0.49255952380952384]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.640625, 0.5878962536023055, 0.5409356725146199, 0.5029673590504451]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6452074391988555, 0.5921625544267054, 0.5449189985272459, 0.5067264573991032]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6494252873563219, 0.597667638483965, 0.5517751479289941, 0.515015015015015]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6428571428571429, 0.5855072463768116, 0.5367647058823529, 0.49850746268656715]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.642346208869814, 0.5834542815674891, 0.5331369661266568, 0.4947683109118087]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6413199426111909, 0.5836972343522562, 0.5361890694239291, 0.4992503748125937]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6436781609195402, 0.5874635568513119, 0.5399408284023669, 0.5015015015015015]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6363636363636364, 0.5821325648414986, 0.533625730994152, 0.49554896142433236]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6381766381766382, 0.5838150289017341, 0.5351906158357771, 0.49702380952380953]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6381766381766382, 0.5852601156069365, 0.5381231671554252, 0.5]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.634180790960452, 0.5830945558739254, 0.5377906976744186, 0.5014749262536873]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6338028169014085, 0.5828571428571429, 0.5391304347826087, 0.5029411764705882]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6293706293706294, 0.5787234042553191, 0.5352517985611511, 0.4992700729927007]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6286509040333796, 0.5825105782792666, 0.542203147353362, 0.5094339622641509]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6282940360610264, 0.5836849507735584, 0.5449358059914408, 0.5137481910274964]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6248275862068966, 0.5804195804195804, 0.5418439716312057, 0.5107913669064749]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6248275862068966, 0.5804195804195804, 0.5418439716312057, 0.5107913669064749]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6256906077348067, 0.5812324929971989, 0.5426136363636364, 0.5115273775216138]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6248275862068966, 0.5804195804195804, 0.5418439716312057, 0.5107913669064749]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.627939142461964, 0.5834502103786816, 0.5448079658605974, 0.5137085137085137]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6270718232044199, 0.5826330532212886, 0.5440340909090909, 0.5129682997118156]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.624484181568088, 0.5801952580195258, 0.5417256011315418, 0.5107604017216643]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6262068965517241, 0.5818181818181818, 0.5432624113475177, 0.5122302158273381]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6296809986130375, 0.5850914205344585, 0.5463623395149786, 0.5151953690303908]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6222527472527473, 0.5766016713091922, 0.536723163841808, 0.504297994269341]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6248275862068966, 0.579020979020979, 0.5390070921985816, 0.5064748201438849]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6256906077348067, 0.5798319327731093, 0.5397727272727273, 0.5072046109510087]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6239669421487604, 0.5782122905027933, 0.5382436260623229, 0.5057471264367817]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[0.6274238227146814, 0.5814606741573034, 0.5413105413105413, 0.5086705202312138]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"}]},{"cell_type":"code","source":"# alpaca_prompt = Copied from above\nhypotheses_list=[]\nreference_list=[]\nfor i in range(len(val_dataset)):\n  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n  inputs = tokenizer(\n  [\n      alpaca_prompt.format(\n          df['Schema'][30+i], # instruction\n          df['Question'][30+i], # input\n          \"\", # output - leave this blank for generation!\n      )\n  ], return_tensors = \"pt\").to(\"cuda\")\n\n\n  outputs = model.generate(**inputs, max_new_tokens = 500, use_cache = False,temperature=0)\n  x=tokenizer.batch_decode(outputs)[0]\n  resp_index =x.find('### Response:')\n\n  hypotheses_list.append(tokenizer.batch_decode(outputs)[0][resp_index+len(\"### Response:\"):-len(\"<|endoftext|>\")])\n  reference_list.append(df['Answer'][30+i])","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:18:06.366589Z","iopub.execute_input":"2024-06-15T17:18:06.366966Z","iopub.status.idle":"2024-06-15T17:18:47.929029Z","shell.execute_reply.started":"2024-06-15T17:18:06.366937Z","shell.execute_reply":"2024-06-15T17:18:47.927858Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\ndef calculate_bleu(hypotheses_list, reference_list):\n    # Convert references_list to a list of lists of references (required format for NLTK's corpus_bleu)\n    references = [[ref.split()] for ref in reference_list]\n\n    # Convert hypotheses_list to a list of lists of hypotheses (required format for NLTK's corpus_bleu)\n    hypotheses = [hyp.split() for hyp in hypotheses_list]\n\n    # Compute BLEU scores\n    bleu_scores = corpus_bleu(references, hypotheses)\n\n    return bleu_scores\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:19:55.144553Z","iopub.execute_input":"2024-06-15T17:19:55.144935Z","iopub.status.idle":"2024-06-15T17:19:55.152579Z","shell.execute_reply.started":"2024-06-15T17:19:55.144904Z","shell.execute_reply":"2024-06-15T17:19:55.151392Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"results = bleu.compute(predictions=hypotheses_list,references=reference_list)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:19:48.572429Z","iopub.execute_input":"2024-06-15T17:19:48.573323Z","iopub.status.idle":"2024-06-15T17:19:48.593686Z","shell.execute_reply.started":"2024-06-15T17:19:48.573288Z","shell.execute_reply":"2024-06-15T17:19:48.592853Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:19:48.960694Z","iopub.execute_input":"2024-06-15T17:19:48.961045Z","iopub.status.idle":"2024-06-15T17:19:48.969665Z","shell.execute_reply.started":"2024-06-15T17:19:48.961016Z","shell.execute_reply":"2024-06-15T17:19:48.968517Z"},"trusted":true},"execution_count":92,"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"{'bleu': 0.7326938979970893,\n 'precisions': [0.8156934306569343,\n  0.7509293680297398,\n  0.7064393939393939,\n  0.666023166023166],\n 'brevity_penalty': 1.0,\n 'length_ratio': 1.1048387096774193,\n 'translation_length': 548,\n 'reference_length': 496}"},"metadata":{}}]},{"cell_type":"code","source":"bleu_scores = calculate_bleu(hypotheses_list, reference_list)\nprint(\"BLEU Scores:\", bleu_scores)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:20:00.615734Z","iopub.execute_input":"2024-06-15T17:20:00.616759Z","iopub.status.idle":"2024-06-15T17:20:00.626163Z","shell.execute_reply.started":"2024-06-15T17:20:00.616722Z","shell.execute_reply":"2024-06-15T17:20:00.625031Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"BLEU Scores: 0.5515775331536508\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install evaluate\nimport evaluate\n","metadata":{"execution":{"iopub.status.busy":"2024-06-15T15:44:43.221367Z","iopub.execute_input":"2024-06-15T15:44:43.222062Z","iopub.status.idle":"2024-06-15T15:44:57.085499Z","shell.execute_reply.started":"2024-06-15T15:44:43.222025Z","shell.execute_reply":"2024-06-15T15:44:57.084382Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.20.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m869.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:20:30.792362Z","iopub.execute_input":"2024-06-15T17:20:30.793397Z","iopub.status.idle":"2024-06-15T17:20:46.447308Z","shell.execute_reply.started":"2024-06-15T17:20:30.793356Z","shell.execute_reply":"2024-06-15T17:20:46.446264Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=9180bdcd965ed132d235bf5aebbc350ade8991be9fff6fbb74f4ec364ab56fc4\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"bleu=evaluate.load('bleu')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:05:52.842740Z","iopub.execute_input":"2024-06-15T17:05:52.843130Z","iopub.status.idle":"2024-06-15T17:05:53.574386Z","shell.execute_reply.started":"2024-06-15T17:05:52.843101Z","shell.execute_reply":"2024-06-15T17:05:53.573255Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"rouge = evaluate.load('rouge')\n\nresults = rouge.compute(predictions=hypotheses_list,references=reference_list)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:20:46.450440Z","iopub.execute_input":"2024-06-15T17:20:46.450840Z","iopub.status.idle":"2024-06-15T17:20:46.951255Z","shell.execute_reply.started":"2024-06-15T17:20:46.450802Z","shell.execute_reply":"2024-06-15T17:20:46.950125Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:20:46.952564Z","iopub.execute_input":"2024-06-15T17:20:46.952951Z","iopub.status.idle":"2024-06-15T17:20:46.960643Z","shell.execute_reply.started":"2024-06-15T17:20:46.952915Z","shell.execute_reply":"2024-06-15T17:20:46.959605Z"},"trusted":true},"execution_count":98,"outputs":[{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"{'rouge1': 0.8619392684610075,\n 'rouge2': 0.7808592810415564,\n 'rougeL': 0.8367629660559877,\n 'rougeLsum': 0.8574583456571034}"},"metadata":{}}]},{"cell_type":"code","source":"for hyp,ref in zip(hypotheses_list,reference_list):\n    print(hyp)\n    print(\"\\n\")\n    print(ref)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:20:54.388881Z","iopub.execute_input":"2024-06-15T17:20:54.389267Z","iopub.status.idle":"2024-06-15T17:20:54.401364Z","shell.execute_reply.started":"2024-06-15T17:20:54.389239Z","shell.execute_reply":"2024-06-15T17:20:54.400361Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"\nMATCH (c:Client)<-[:HAS_CLIENT]-(i:Invoice)\nWITH c.name AS ClientName, COUNT(i) AS InvoiceCount\nWHERE InvoiceCount > 1\nRETURN ClientName, InvoiceCount\n\n\nMATCH (c:Client)<-[:HAS_CLIENT]-(i:Invoice)\nWITH c.name AS ClientName, COUNT(i) AS InvoiceCount\nWHERE InvoiceCount > 1\nRETURN ClientName, InvoiceCount\n\nMATCH (i:Invoice)-[:HAS_PRODUCT]->(p:ProductDescription)-[:HAS_UNIT_PRICE]->(u:UnitPrice)-[:HAS_TOTAL]->(t:Total)\nWHERE COUNT(toInteger(p.name)) > 1\nRETURN i.name AS InvoiceName, p.name AS ProductDescription, u.name AS UnitPrice, t.name AS Total\n\n\nMATCH (i:Invoice)-[:HAS_PRODUCT]->(p:ProductDescription), \n      (i)-[:HAS_TOTAL_AMOUNT]->(t:TotalAmount)\nWITH i, COUNT(p) AS ProductCount, toInteger(t.name) AS TotalAmount\nWITH AVG(TotalAmount) AS AvgAmount, i, ProductCount, TotalAmount\nWHERE ProductCount > 1 AND TotalAmount > AvgAmount\nRETURN i.name AS InvoiceName, ProductCount, TotalAmount\n\nMATCH (i:Invoice)-[:HAS_CLIENT]->(c:Client)\nRETURN i.name AS InvoiceName, c.name AS ClientName\n\n\nMATCH (i:Invoice)-[:HAS_CLIENT]->(c:Client)\nRETURN i.name AS InvoiceName, c.name AS ClientName\n\nMATCH (i:Invoice)-[:HAS_TOTAL_AMOUNT]->(t:TotalAmount), \n      (i)-[:HAS_PRODUCT]->(p:ProductDescription), \n      (p)-[:HAS_TOTAL]->(t:Total)\nRETURN i.name AS InvoiceName, t.name AS TotalAmount, p.name AS ProductDescription, t.name AS Total\n\n\nMATCH (i:Invoice)-[:HAS_TOTAL_AMOUNT]->(t:TotalAmount), \n      (i)-[:HAS_PRODUCT]->(p:ProductDescription)-[:HAS_TOTAL]->(pt:Total)\nRETURN i.name AS InvoiceName, t.name AS TotalAmount, p.name AS ProductDescription, pt.name AS ProductTotal\n\nMATCH (s:Seller)\nRETURN s.name AS SellerName\n\n\nMATCH (s:Seller)\nRETURN s.name AS SellerName\n\nMATCH (i:Invoice)-[:HAS_CLIENT]->(c:Client), \n      (i:Invoice)-[:HAS_SELLER]->(s:Seller)\nRETURN i.name AS InvoiceName, \n       c.name AS ClientName, \n       s.name AS SellerName\n\n\nMATCH (i:Invoice)-[:HAS_CLIENT]->(c:Client), (i)-[:HAS_SELLER]->(s:Seller)\nRETURN i.name AS InvoiceName, c.name AS ClientName, s.name AS SellerName\n\nMATCH (invoice:Invoice)-[:HAS_PRODUCT]->(product:ProductDescription)-[:HAS_QUANTITY]->(quantity:Quantity)\nOPTIONAL MATCH (product)-[:HAS_UNIT_PRICE]->(unitPrice:UnitPrice)\nOPTIONAL MATCH (product)-[:HAS_TOTAL]->(total:Total)\nRETURN COUNT(toInteger(product.name)) AS TotalUnitsSold\n\n\nMATCH (product:ProductDescription {name: 'ProductX'})-[:HAS_QUANTITY]->(quantity:Quantity)\nRETURN sum(toInteger(quantity.name)) AS TotalQuantitySold\n\nMATCH (i:Invoice)-[:HAS_PRODUCT]->(p:ProductDescription)-[:HAS_QUANTITY]->(q:Quantity)\nRETURN i.name AS InvoiceName, p.name AS ProductDescription, q.name AS Quantity\n\n\nMATCH (i:Invoice)-[:HAS_PRODUCT]->(p:ProductDescription)-[:HAS_QUANTITY]->(q:Quantity)\nRETURN i.name AS InvoiceName, p.name AS ProductDescription, q.name AS Quantity\n\nMATCH (i:Invoice)-[:HAS_TOTAL_AMOUNT]->(t:TotalAmount)\nWHERE toFloat(t.name) > {value: 1000}\nRETURN i.name AS InvoiceID, t.name AS TotalAmount\n\n\nMATCH (i:Invoice)-[:HAS_TOTAL_AMOUNT]->(t:TotalAmount)\nWHERE toInteger(t.name) > 1000\nRETURN i.name AS InvoiceName, t.name AS TotalAmount\n\nMATCH (i:Invoice)-[:HAS_PRODUCT]->(p:ProductDescription)-[:HAS_TOTAL]->(t:Total)\nRETURN i.name AS InvoiceName, p.name AS ProductDescription, t.name AS Total\n\n\nMATCH (i:Invoice)-[:HAS_PRODUCT]->(p:ProductDescription)-[:HAS_TOTAL]->(t:Total)\nWITH i, SUM(toInteger(t.name)) AS CalculatedTotal\nRETURN i.name AS InvoiceName, CalculatedTotal\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\") # Local saving\ntokenizer.save_pretrained(\"lora_model\")","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:36:57.430049Z","iopub.execute_input":"2024-06-15T17:36:57.430454Z","iopub.status.idle":"2024-06-15T17:36:57.973226Z","shell.execute_reply.started":"2024-06-15T17:36:57.430427Z","shell.execute_reply":"2024-06-15T17:36:57.971978Z"},"trusted":true},"execution_count":101,"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"('lora_model/tokenizer_config.json',\n 'lora_model/special_tokens_map.json',\n 'lora_model/vocab.json',\n 'lora_model/merges.txt',\n 'lora_model/added_tokens.json',\n 'lora_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(\"alterf/cypher2text\", token = \"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\") # Online saving\ntokenizer.push_to_hub(\"alterf/cypher2text\", token = \"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\") # Online saving","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:46:31.071425Z","iopub.execute_input":"2024-06-15T17:46:31.072292Z","iopub.status.idle":"2024-06-15T17:46:36.658170Z","shell.execute_reply.started":"2024-06-15T17:46:31.072261Z","shell.execute_reply":"2024-06-15T17:46:36.657037Z"},"trusted":true},"execution_count":103,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/555 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c33a33d8234848818fd8663647b5da6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/35.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7582763344b14c6198132ff6a11d7e6a"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/alterf/cypher2text\n","output_type":"stream"}]}]}